{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skJOAVI_JqTt"
      },
      "source": [
        "# Embedding Models Server for RAG Evaluation\n",
        "\n",
        "This notebook sets up an API endpoint for embedding models that can be accessed remotely. It leverages Colab's A100 GPU for computation and exposes the embeddings through a simple Flask API, made public using ngrok."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJTlghpBJqTv",
        "outputId": "59112066-cee5-4631-e179-98bfa9d68560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/163.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.0/135.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install flask pyngrok transformers sentence-transformers FlagEmbedding flask-cloudflared -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR0sYs-2JqTv",
        "outputId": "893e5350-d4cd-4396-da49-c6847c87e3d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKHH5hxdJqTw"
      },
      "outputs": [],
      "source": [
        "# Define your authentication token (change this to a secure value)\n",
        "AUTH_TOKEN = \"your_secure_token_here\"\n",
        "\n",
        "# Optional: Store as environment variable\n",
        "import os\n",
        "os.environ['AUTH_TOKEN'] = AUTH_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlJdMrB1JqTw"
      },
      "outputs": [],
      "source": [
        "# Load embedding models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import time\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "\n",
        "# Initialize dictionary to store models\n",
        "models = {}\n",
        "\n",
        "# Function to load a regular sentence transformer model\n",
        "def load_model(name, model_id):\n",
        "    print(f\"Loading {name} model...\")\n",
        "    start_time = time.time()\n",
        "    model = SentenceTransformer(model_id)\n",
        "    # Move to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.to(torch.device(\"cuda\"))\n",
        "    print(f\"Loaded {name} in {time.time() - start_time:.2f} seconds\")\n",
        "    return model\n",
        "\n",
        "# Function to load Nomic model\n",
        "def load_nomic_model():\n",
        "    print(\"Loading Nomic Embed model...\")\n",
        "    start_time = time.time()\n",
        "    model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
        "    # Move to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.to(torch.device(\"cuda\"))\n",
        "    print(f\"Loaded Nomic Embed in {time.time() - start_time:.2f} seconds\")\n",
        "    return model\n",
        "\n",
        "# Function to load BGE-M3 model\n",
        "def load_bge_m3_model():\n",
        "    print(\"Loading BGE-M3 model...\")\n",
        "    start_time = time.time()\n",
        "    model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
        "    print(f\"Loaded BGE-M3 in {time.time() - start_time:.2f} seconds\")\n",
        "    return model\n",
        "\n",
        "# Load models\n",
        "models[\"e5\"] = load_model(\"E5\", \"intfloat/multilingual-e5-large\")\n",
        "models[\"nomic\"] = load_nomic_model()\n",
        "models[\"bge-m3\"] = load_bge_m3_model()\n",
        "\n",
        "# Test embedding generation\n",
        "test_text = \"This is a test sentence for embedding models.\"\n",
        "print(\"Testing models with a sample text...\")\n",
        "\n",
        "# Test E5\n",
        "e5_embedding = models[\"e5\"].encode(test_text)\n",
        "print(f\"E5 embedding shape: {e5_embedding.shape}\")\n",
        "\n",
        "# Test Nomic\n",
        "nomic_embedding = models[\"nomic\"].encode([test_text], prompt_name=\"passage\")\n",
        "print(f\"Nomic embedding shape: {nomic_embedding.shape}\")\n",
        "\n",
        "# Test BGE-M3\n",
        "bge_m3_output = models[\"bge-m3\"].encode(test_text)\n",
        "bge_m3_embedding = bge_m3_output['dense_vecs']\n",
        "print(f\"BGE-M3 embedding shape: {len(bge_m3_embedding) if isinstance(bge_m3_embedding, list) else bge_m3_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbfjXFvTJqTw"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cloudflared import run_with_cloudflared\n",
        "import numpy as np\n",
        "import time\n",
        "import traceback\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Create Flask app\n",
        "app = Flask(__name__)\n",
        "run_with_cloudflared(app)  # This is key - it adds the Cloudflare tunnel functionality\n",
        "\n",
        "# Authentication middleware\n",
        "@app.before_request\n",
        "def authenticate():\n",
        "    if request.path == '/health':\n",
        "        return None  # Skip auth for health check\n",
        "    auth_header = request.headers.get('Authorization')\n",
        "    if not auth_header or auth_header != f\"Bearer {AUTH_TOKEN}\":\n",
        "        return jsonify({\"error\": \"Unauthorized\"}), 401\n",
        "\n",
        "# Health check endpoint (no auth required)\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\"status\": \"healthy\", \"models\": list(models.keys())})\n",
        "\n",
        "# Embedding endpoint\n",
        "@app.route('/embed', methods=['POST'])\n",
        "def embed():\n",
        "    try:\n",
        "        data = request.json\n",
        "\n",
        "        # Strictly require all parameters\n",
        "        if 'model' not in data:\n",
        "            return jsonify({\"error\": \"Missing 'model' in request\"}), 400\n",
        "\n",
        "        if 'texts' not in data:\n",
        "            return jsonify({\"error\": \"Missing 'texts' in request\"}), 400\n",
        "\n",
        "        model_name = data['model']\n",
        "        texts = data['texts']\n",
        "\n",
        "        # Validate model\n",
        "        if model_name not in models:\n",
        "            return jsonify({\"error\": f\"Model '{model_name}' not found. Available models: {list(models.keys())}\"}), 400\n",
        "\n",
        "        # Handle single text or list of texts\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        # Get the model\n",
        "        model = models[model_name]\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Model-specific encoding logic\n",
        "        if model_name == \"bge-m3\":\n",
        "            # BGE-M3 requires extracting the dense vectors from the returned dict\n",
        "            result = model.encode(texts)\n",
        "            embeddings = result['dense_vecs']\n",
        "\n",
        "        elif model_name == \"nomic\":\n",
        "            # Nomic Embed requires task instruction\n",
        "            is_query = data.get('is_query', False)  # Default to document mode for Nomic\n",
        "\n",
        "            # Use SentenceTransformer's prompt_name parameter\n",
        "            prompt_name = \"query\" if is_query else \"passage\"\n",
        "            embeddings = model.encode(texts, prompt_name=prompt_name)\n",
        "\n",
        "        elif model_name.startswith(\"e5\"):\n",
        "            prefix = \"query: \" if data.get(\"is_query\", False) else \"passage: \"\n",
        "            texts = [prefix + t for t in texts]\n",
        "            embeddings = model.encode(texts)\n",
        "        # Convert to list for JSON serialization\n",
        "        if isinstance(embeddings, np.ndarray):\n",
        "            embeddings = embeddings.tolist()\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        return jsonify({\n",
        "            \"embeddings\": embeddings,\n",
        "            \"model\": model_name,\n",
        "            \"processing_time\": processing_time,\n",
        "            \"dimensions\": len(embeddings[0]) if embeddings else 0\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        error_traceback = traceback.format_exc()\n",
        "        logger.error(f\"Error: {str(e)}\\n{error_traceback}\")\n",
        "        return jsonify({\n",
        "            \"error\": str(e),\n",
        "            \"traceback\": error_traceback\n",
        "        }), 500\n",
        "\n",
        "# Main block to run the app\n",
        "if __name__ == '__main__':\n",
        "    # Print information before starting the app\n",
        "    print(f\"\\n\\n===== IMPORTANT =====\")\n",
        "    print(f\"Auth Token: {AUTH_TOKEN}\")\n",
        "    print(f\"====================\\n\")\n",
        "    print(f\"The Cloudflare Tunnel URL will appear below once the server starts\")\n",
        "    print(f\"Your server will remain active as long as this notebook is running\")\n",
        "\n",
        "    # Run the app - this starts both Flask and the Cloudflare tunnel\n",
        "    # You can customize ports if needed\n",
        "    app.run(host='0.0.0.0', port=5000) # metrics_port will be chosen randomly\n",
        "\n",
        "    # Note: execution will block here until the app is stopped"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
